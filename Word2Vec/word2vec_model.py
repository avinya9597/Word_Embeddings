# -*- coding: utf-8 -*-
"""Word2Vec_Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Xy5iP7YdYGjmP9Rm0WnoRQZwrjYB_YQ0
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import re
import string
import nltk
nltk.download('punkt')
#stopwords.remove("not")
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
wn = nltk.WordNetLemmatizer()
nltk.download('stopwords')

df = pd.read_csv('/content/FinalData.csv')

df.sample(frac=1).head()

df.isnull().sum()

df.shape

df.dropna(inplace=True)

"""## Data Cleaning : Text Preprocessing

###Remove punctuations, stopwords
---


"""

from gensim.parsing.preprocessing import remove_stopwords, STOPWORDS
def remove_punct(text):
    text  = "".join([char for char in text if char not in string.punctuation])
    text = re.sub('[0-9]+', '', text)
    return text

def tokenization(text):
    text = re.split('\W+', text)
    return text

def remove_stopwords(text):
    text = [word for word in text if word not in STOPWORDS]
    return text

def lemmatizer(text):
    text = [wn.lemmatize(word) for word in text]
    return text

ps = nltk.PorterStemmer()

def stemming(text):
    text = [ps.stem(word) for word in text]
    return text

df['quote_punct_removed'] = df['Description'].apply(lambda x: remove_punct(x))
df["quote_tokenized"] = df["Description"].apply(lambda x: tokenization(x))
df["quote_stop_word"] = df["quote_tokenized"].apply(lambda x: remove_stopwords(x))
df["quotes_cleaned"] = df["quote_stop_word"].apply(lambda x: " ".join(x))

df.head(2)

texts = []
texts = df['quotes_cleaned'].to_list()

len(texts)

"""## Training the word2vec model"""

from gensim.models import Word2Vec

sentences = [line.split() for line in texts]

w2v =Word2Vec(sentences, window=5, workers=4, min_count=5)

print(sentences[20:25])

words = list(w2v.wv.index_to_key)

words[0:55]

print(len(words))

"""### How the word vector looks like"""

print( w2v.wv['Apple'] )

"""### Check similarity scores between words"""

w2v.wv.similarity('generative', 'OpenAI')

w2v.wv.similarity('recession', 'economy')

"""### Check for similar words"""

print(w2v.wv.most_similar('recession'))

"""### Similarly, we can use the same function to find analogies of the form: if x:y, then z:?. Here we enter the known relation x,y in the positive parameter, and the term whoâ€™s analogy has to be found in the negative parameter."""

print(w2v.wv.most_similar(positive=['Apple', 'company'], negative=['chatbot']))